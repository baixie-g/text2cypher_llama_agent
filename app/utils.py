import json
import logging
from datetime import datetime
from typing import Any, Dict, List

from fastapi import Request
from jinja2 import pass_context


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('llm_interactions.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

class LLMLogger:
    """LLMäº¤äº’æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self):
        self.logger = logging.getLogger('LLM_Logger')
        self.interaction_count = 0
    
    def log_prompt(self, step_name: str, prompt_content: Any, context: Dict[str, Any] = None):
        """è®°å½•å‘é€ç»™LLMçš„æç¤ºè¯"""
        self.interaction_count += 1
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        print(f"\n{'='*80}")
        print(f"ğŸ”µ LLMäº¤äº’ #{self.interaction_count} - {step_name}")
        print(f"â° æ—¶é—´: {timestamp}")
        print(f"{'='*80}")
        
        if isinstance(prompt_content, list):
            print("ğŸ“¤ å‘é€ç»™LLMçš„æç¤ºè¯:")
            for i, msg in enumerate(prompt_content):
                if isinstance(msg, tuple) and len(msg) == 2:
                    role, content = msg
                    print(f"\n--- {role.upper()} ---")
                    print(content)
                else:
                    print(f"\n--- æ¶ˆæ¯ {i+1} ---")
                    print(msg)
        else:
            print("ğŸ“¤ å‘é€ç»™LLMçš„æç¤ºè¯:")
            print(prompt_content)
        
        if context:
            print(f"\nğŸ“‹ ä¸Šä¸‹æ–‡ä¿¡æ¯:")
            print(json.dumps(context, ensure_ascii=False, indent=2))
        
        print(f"{'='*80}\n")
        
        # åŒæ—¶è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
        self.logger.info(f"LLMäº¤äº’ #{self.interaction_count} - {step_name} - æç¤ºè¯å·²å‘é€")
    
    def log_response(self, step_name: str, response_content: str, context: Dict[str, Any] = None):
        """è®°å½•LLMçš„å®Œæ•´å›åº”"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        print(f"\n{'='*80}")
        print(f"ğŸŸ¢ LLMå›åº” #{self.interaction_count} - {step_name}")
        print(f"â° æ—¶é—´: {timestamp}")
        print(f"{'='*80}")
        print("ğŸ“¥ LLMå®Œæ•´å›åº”:")
        print(response_content)
        
        if context:
            print(f"\nğŸ“‹ ä¸Šä¸‹æ–‡ä¿¡æ¯:")
            print(json.dumps(context, ensure_ascii=False, indent=2))
        
        print(f"{'='*80}\n")
        
        # åŒæ—¶è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
        self.logger.info(f"LLMäº¤äº’ #{self.interaction_count} - {step_name} - å›åº”å·²æ¥æ”¶")
    
    def log_workflow_step(self, step_name: str, message: str, data: Any = None):
        """è®°å½•å·¥ä½œæµæ­¥éª¤ä¿¡æ¯"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        print(f"\n{'='*60}")
        print(f"ğŸ”„ å·¥ä½œæµæ­¥éª¤: {step_name}")
        print(f"â° æ—¶é—´: {timestamp}")
        print(f"ğŸ’¬ æ¶ˆæ¯: {message}")
        
        if data:
            print(f"ğŸ“Š æ•°æ®:")
            if isinstance(data, str):
                print(data)
            else:
                print(json.dumps(data, ensure_ascii=False, indent=2))
        
        print(f"{'='*60}\n")
        
        # åŒæ—¶è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
        self.logger.info(f"å·¥ä½œæµæ­¥éª¤: {step_name} - {message}")

# å…¨å±€LLMæ—¥å¿—è®°å½•å™¨å®ä¾‹
llm_logger = LLMLogger()

def get_llm_logger() -> LLMLogger:
    """è·å–LLMæ—¥å¿—è®°å½•å™¨å®ä¾‹"""
    return llm_logger


def get_optimized_schema(graph_store, exclude_types: List[str] = None) -> str:
    """
    è·å–ä¼˜åŒ–çš„schemaä¿¡æ¯ï¼ŒåªåŒ…å«node_propså’Œrelationships
    è¿‡æ»¤æ‰Entityæ ‡ç­¾å’Œå…¶ä»–ä¸éœ€è¦çš„ä¿¡æ¯
    """
    if exclude_types is None:
        exclude_types = ["Entity", "Actor", "Director"]
    else:
        exclude_types.extend(["Entity"])
    
    # è·å–åŸå§‹schema
    schema = graph_store.get_schema()
    
    # è¿‡æ»¤node_propsï¼Œæ’é™¤Entityå’Œå…¶ä»–æŒ‡å®šç±»å‹
    filtered_node_props = {}
    for node_type, props in schema.get("node_props", {}).items():
        if node_type not in exclude_types:
            # åªä¿ç•™å±æ€§åç§°ï¼Œä¸åŒ…å«ç±»å‹ä¿¡æ¯
            if isinstance(props, list):
                # å¦‚æœæ˜¯å­—å…¸åˆ—è¡¨æ ¼å¼
                prop_names = [prop.get('property', prop) if isinstance(prop, dict) else prop for prop in props]
            else:
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²åˆ—è¡¨æ ¼å¼
                prop_names = props if isinstance(props, list) else []
            filtered_node_props[node_type] = prop_names
    
    # è¿‡æ»¤relationshipsï¼Œæ’é™¤åŒ…å«Entityçš„å…³ç³»
    filtered_relationships = []
    for rel in schema.get("relationships", []):
        if isinstance(rel, dict):
            # å¦‚æœæ˜¯å­—å…¸æ ¼å¼
            start = rel.get("start", "")
            end = rel.get("end", "")
            rel_type = rel.get("type", "")
        else:
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼Œè§£æå…³ç³»å­—ç¬¦ä¸²
            # æ ¼å¼: (:StartLabel)-[:RelType]->(:EndLabel)
            parts = str(rel).split(")-[:")
            if len(parts) == 2:
                start_part = parts[0].replace("(:", "")
                end_part = parts[1].split("]->(:")
                if len(end_part) == 2:
                    rel_type = end_part[0]
                    end = end_part[1].replace(")", "")
                else:
                    continue
            else:
                continue
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«éœ€è¦æ’é™¤çš„æ ‡ç­¾
        if (start not in exclude_types and 
            end not in exclude_types and 
            rel_type not in exclude_types):
            filtered_relationships.append(rel)
    
    # æ ¼å¼åŒ–è¾“å‡º
    node_props_str = []
    for node_type, props in filtered_node_props.items():
        if props:
            props_str = ", ".join(props)
            node_props_str.append(f"{node_type}: [{props_str}]")
        else:
            node_props_str.append(f"{node_type}: []")
    
    relationships_str = []
    for rel in filtered_relationships:
        if isinstance(rel, dict):
            rel_str = f"(:{rel['start']})-[:{rel['type']}]->(:{rel['end']})"
        else:
            rel_str = str(rel)
        relationships_str.append(rel_str)
    
    # æ„å»ºæœ€ç»ˆçš„schemaå­—ç¬¦ä¸²
    schema_parts = []
    
    if node_props_str:
        schema_parts.append("Node Properties:")
        schema_parts.extend(node_props_str)
    
    if relationships_str:
        schema_parts.append("Relationships:")
        schema_parts.extend(relationships_str)
    
    return "\n".join(schema_parts)


# force HTTPS in jinja templates
@pass_context
def urlx_for(
    context: dict,
    name: str,
    **path_params: Any,
) -> str:
    request: Request = context["request"]
    http_url = request.url_for(name, **path_params)
    if scheme := request.headers.get("x-forwarded-proto"):
        return http_url.replace(scheme=scheme)
    return http_url

from typing import List, Optional
from llama_index.core.embeddings import BaseEmbedding
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F


class BgeEmbedding(BaseEmbedding):
    def __init__(
        self,
        model_path: str,
        device: str = "cuda" if torch.cuda.is_available() else "cpu",
        normalize: bool = True,
        **kwargs
    ):
        super().__init__(**kwargs)
        
        # ç›´æ¥èµ‹å€¼ç»™å®ä¾‹å±æ€§è€Œä¸æ˜¯é€šè¿‡Pydantic
        self.device = device
        self.normalize = normalize
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModel.from_pretrained(model_path).to(self.device)
        self._model_dim = self.model.config.hidden_size  # ä»æ¨¡å‹é…ç½®ä¸­è·å–ç»´åº¦

    @classmethod
    def class_name(cls):
        return "BgeEmbedding"

    def _mean_pooling(self, token_embeddings, attention_mask):
        input_mask_expanded = (
            attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        )
        embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(
            input_mask_expanded.sum(1), min=1e-9
        )
        return embeddings

    def _get_query_embedding(self, query: str):
        inputs = self.tokenizer(
            query,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors="pt",
        ).to(self.device)
        with torch.no_grad():
            outputs = self.model(**inputs)
        embeddings = self._mean_pooling(
            outputs.last_hidden_state, inputs["attention_mask"]
        )
        if self.normalize:
            embeddings = F.normalize(embeddings, p=2, dim=1)
        return embeddings[0].cpu().tolist()

    def _get_text_embedding(self, text: str):
        return self._get_query_embedding(text)

    def _get_text_embeddings(self, texts: List[str]):
        return [self._get_text_embedding(text) for text in texts]

    async def _aget_query_embedding(self, query: str) -> List[float]:
        return self._get_query_embedding(query)

    async def _aget_text_embedding(self, text: str) -> List[float]:
        return self._get_text_embedding(text)

    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:
        return self._get_text_embeddings(texts)

    @property
    def dimensions(self) -> int:
        """è¿”å›åµŒå…¥å‘é‡çš„ç»´åº¦"""
        return self._model_dim