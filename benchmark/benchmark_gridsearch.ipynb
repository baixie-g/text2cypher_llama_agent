{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc911b0b-4e15-401e-9875-e43c25470daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Insert the parent directory of \"app\" into sys.path\n",
    "# so that Python recognizes \"app\" as an importable package.\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85258025-55e3-45db-b6a5-29004264c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # This looks for .env in the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70278d01-fe4a-47fe-91e5-ff91b67a046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from prettytable import PrettyTable\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LlamaIndexLLMWrapper\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness,\n",
    ")\n",
    "from google.generativeai.types import RequestOptions\n",
    "from google.api_core import retry\n",
    "\n",
    "from app.workflows.utils import graph_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4329aafd-3e8c-4acb-abdf-fcedd3f95e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import flows\n",
    "from app.workflows.naive_text2cypher import NaiveText2CypherFlow\n",
    "from app.workflows.naive_text2cypher_retry import NaiveText2CypherRetryFlow\n",
    "from app.workflows.iterative_planner import IterativePlanningFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041de039-c410-417f-bec2-9b92a70cdf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark data\n",
    "test_df = pd.read_csv('test_data.csv', delimiter=\";\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a57e472-52c6-4da0-b6ef-d3122d5c4d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_flow_llm_combination(flow_name, flow, llm_name, llm, test_df, graph_store):\n",
    "    results = []\n",
    "    latencies = []\n",
    "    ground_truth = []\n",
    "\n",
    "    flow_instance = flow(llm=llm, timeout=90)\n",
    "    \n",
    "    for i, row in test_df.iterrows():\n",
    "        question = row['Question']\n",
    "        \n",
    "        start = time.time()\n",
    "        try:\n",
    "            data = await flow_instance.run(input=question)\n",
    "        except:\n",
    "            data = {\"answer\": \"timeout/error\", \"question\": question}\n",
    "        end = time.time()\n",
    "        latencies.append(end - start)\n",
    "        results.append(data)\n",
    "            \n",
    "        try:\n",
    "            ground_truth.append(graph_store.structured_query(row['Cypher']))\n",
    "        except Exception as e:\n",
    "            ground_truth.append(\"missing\")\n",
    "    # Create evaluation dataset\n",
    "    df = pd.DataFrame(results)\n",
    "    df['ground_truth'] = [str(el) for el in ground_truth]\n",
    "    df['latencies'] = latencies\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    # Run evaluation\n",
    "    result = evaluate(\n",
    "        dataset,\n",
    "        metrics=[answer_relevancy],\n",
    "        llm=LlamaIndexLLMWrapper(OpenAI(model=\"gpt-4o-2024-11-20\", temperature=0))\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'answer_relevancy': result['answer_relevancy'],\n",
    "        'avg_latency': sum(latencies) / len(latencies)\n",
    "    }\n",
    "\n",
    "async def run_grid_search(\n",
    "    flows: List[callable],\n",
    "    llms: List[object],\n",
    "    test_df: pd.DataFrame,\n",
    "    graph_store: object\n",
    "):\n",
    "    results = []\n",
    "    \n",
    "    for flow in flows:\n",
    "        for llm_name, llm in llms:\n",
    "            print(f\"\\nEvaluating {flow.__name__} with {llm_name}\")\n",
    "            \n",
    "            result = await evaluate_flow_llm_combination(\n",
    "                flow_name=flow.__name__,\n",
    "                flow=flow,\n",
    "                llm_name=llm_name,\n",
    "                llm=llm,\n",
    "                test_df=test_df,\n",
    "                graph_store=graph_store\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'flow': flow.__name__,\n",
    "                'llm': llm_name,\n",
    "                **result\n",
    "            })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a7173-eb21-4b46-b78e-c6dbfcab1c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "flows = [\n",
    "    IterativePlanningFlow,\n",
    "    NaiveText2CypherFlow,\n",
    "    NaiveText2CypherRetryFlow,\n",
    "]  # Add your flows\n",
    "\n",
    "google_retry = dict(retry=retry.Retry(initial=0.1, multiplier=2, timeout=61))\n",
    "llms = [\n",
    "    (\"1.5pro\", Gemini(model=\"models/gemini-1.5-pro\", temperature=0, request_options=google_retry)),\n",
    "    #(\"1.5flash\", Gemini(model=\"models/gemini-1.5-flash\", temperature=0, request_options=google_retry)),\n",
    "    #(\"2.0flash\", Gemini(model=\"models/gemini-2.0-flash-exp\", temperature=0)), # rate limits \n",
    "    (\"gpt-4o\", OpenAI(model=\"gpt-4o\", temperature=0)),\n",
    "    #(\"gpt-4o-mini\", OpenAI(model=\"gpt-4o-mini\", temperature=0)),\n",
    "    #(\"o1\", OpenAI(model=\"o1-preview\", temperature=0)), no tools\n",
    "    #(\"o1-mini\", OpenAI(model=\"o1-mini\", temperature=0)), no tools\n",
    "]  # Add your LLMs\n",
    "\n",
    "results = await run_grid_search(\n",
    "    flows=flows, llms=llms, test_df=test_df, graph_store=graph_store\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4961dd5-e34c-4698-8e10-b85f9ae7a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results: List[Dict]):\n",
    "    # Create table\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Flow\", \"LLM\", \"Answer Relevancy\", \"Avg Latency (s)\"]\n",
    "    \n",
    "    # Sort results by answer relevancy\n",
    "    sorted_results = sorted(results, key=lambda x: x['answer_relevancy'], reverse=True)\n",
    "    \n",
    "    # Add rows\n",
    "    for result in sorted_results:\n",
    "        # Handle different data types for answer_relevancy\n",
    "        if isinstance(result['answer_relevancy'], list):\n",
    "            answer_relevancy = sum(result['answer_relevancy']) / len(result['answer_relevancy'])\n",
    "        else:\n",
    "            answer_relevancy = result['answer_relevancy']\n",
    "            \n",
    "        table.add_row([\n",
    "            result['flow'],\n",
    "            result['llm'],\n",
    "            f\"{answer_relevancy:.3f}\" if isinstance(answer_relevancy, (float, int)) else str(answer_relevancy),\n",
    "            f\"{result['avg_latency']:.2f}\"\n",
    "        ])\n",
    "    \n",
    "    print(\"\\nGrid Search Results:\")\n",
    "    print(table)\n",
    "    \n",
    "print_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f48ba6-7981-4628-9e8c-174f7e145cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb40af-339b-45fb-b17f-3036d155f305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
